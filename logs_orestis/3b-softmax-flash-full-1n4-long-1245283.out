===== 3B SOFTMAX + FLASHATTN (FULL, 1N4G, LONG) =====
using world size: 4, data-parallel size: 4, context-parallel size: 1, hierarchical context-parallel sizes: Nonetensor-model-parallel size: 1, encoder-tensor-model-parallel size: 0, pipeline-model-parallel size: 1, encoder-pipeline-model-parallel size: 0
WARNING: overriding default arguments for tokenizer_type:GPT2BPETokenizer                        with tokenizer_type:HuggingFaceTokenizer
accumulate and all-reduce gradients in fp32 for bfloat16 data type.
using torch.bfloat16 for parameters ...
Exit trigger setup! run `touch /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/logs/3b-softmax-flash-full-1n4-long-1245283/exit` to stop training
Save trigger setup! run `touch /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/logs/3b-softmax-flash-full-1n4-long-1245283/save` to save a checkpoint
------------------------ arguments ------------------------
  account_for_embedding_in_pipeline_split ......... False
  account_for_loss_in_pipeline_split .............. False
  accumulate_allreduce_grads_in_fp32 .............. True
  adam_beta1 ...................................... 0.9
  adam_beta2 ...................................... 0.999
  adam_eps ........................................ 1e-08
  add_bias_linear ................................. True
  add_position_embedding .......................... True
  add_qkv_bias .................................... True
  ademamix_alpha .................................. 5
  ademamix_alpha_warmup ........................... -1
  ademamix_beta3 .................................. 0.999
  ademamix_beta3_warmup ........................... -1
  adlr_autoresume ................................. False
  adlr_autoresume_interval ........................ 1000
  align_grad_reduce ............................... True
  align_param_gather .............................. False
  app_tag_run_name ................................ None
  app_tag_run_version ............................. 0.0.0
  apply_layernorm_1p .............................. False
  apply_query_key_layer_scaling ................... False
  apply_residual_connection_post_layernorm ........ False
  apply_rope_fusion ............................... False
  async_save ...................................... None
  async_tensor_model_parallel_allreduce ........... True
  attention_backend ............................... AttnBackend.auto
  attention_dropout ............................... 0.1
  attention_softmax_in_fp32 ....................... False
  attn_layernorm .................................. True
  auto_detect_ckpt_format ......................... False
  barrier_with_L1_time ............................ True
  bert_binary_head ................................ True
  bert_embedder_type .............................. megatron
  bert_load ....................................... None
  bf16 ............................................ True
  bias_dropout_fusion ............................. True
  bias_gelu_fusion ................................ True
  bias_swiglu_fusion .............................. True
  biencoder_projection_dim ........................ 0
  biencoder_shared_query_context_model ............ False
  block_data_path ................................. None
  calc_ft_timeouts ................................ False
  calculate_per_token_loss ........................ False
  check_for_nan_in_loss_and_grad .................. True
  check_for_spiky_loss ............................ False
  check_weight_hash_across_dp_replicas_interval ... None
  ckpt_assume_constant_structure .................. False
  ckpt_convert_format ............................. None
  ckpt_convert_save ............................... None
  ckpt_convert_update_legacy_dist_opt_format ...... False
  ckpt_format ..................................... torch_dist
  ckpt_fully_parallel_load ........................ False
  ckpt_fully_parallel_save ........................ True
  ckpt_fully_parallel_save_deprecated ............. False
  ckpt_step ....................................... None
  classes_fraction ................................ 1.0
  clip_grad ....................................... 1.0
  clone_scatter_output_in_embedding ............... True
  config_logger_dir ............................... 
  consumed_train_samples .......................... 0
  consumed_valid_samples .......................... 0
  context_parallel_size ........................... 1
  cp_comm_type .................................... ['p2p']
  create_attention_mask_in_dataloader ............. True
  cross_entropy_loss_fusion ....................... False
  cuda_graph_warmup_steps ......................... 2
  data_args_path .................................. None
  data_cache_path ................................. None
  data_parallel_random_init ....................... False
  data_parallel_size .............................. 4
  data_path ....................................... ['/users/ooikonomou/scratch/Megatron-LM-new-attention/datasets/train_data_meg_text_document']
  data_per_class_fraction ......................... 1.0
  data_sharding ................................... True
  dataloader_type ................................. single
  ddp_average_in_collective ....................... False
  ddp_bucket_size ................................. None
  decoder_first_pipeline_num_layers ............... None
  decoder_last_pipeline_num_layers ................ None
  decoder_num_layers .............................. None
  decoder_seq_length .............................. None
  decoupled_lr .................................... None
  decoupled_min_lr ................................ None
  decrease_batch_size_if_needed ................... False
  defer_embedding_wgrad_compute ................... False
  deprecated_use_mcore_models ..................... False
  deterministic_mode .............................. False
  dino_bottleneck_size ............................ 256
  dino_freeze_last_layer .......................... 1
  dino_head_hidden_size ........................... 2048
  dino_local_crops_number ......................... 10
  dino_local_img_size ............................. 96
  dino_norm_last_layer ............................ False
  dino_teacher_temp ............................... 0.07
  dino_warmup_teacher_temp ........................ 0.04
  dino_warmup_teacher_temp_epochs ................. 30
  disable_straggler_on_startup .................... False
  dist_ckpt_format_deprecated ..................... None
  dist_ckpt_strictness ............................ assume_ok_unexpected
  distribute_saved_activations .................... False
  distributed_backend ............................. nccl
  distributed_timeout_minutes ..................... 10
  embedding_path .................................. None
  empty_unused_memory_level ....................... 0
  enable_cuda_graph ............................... False
  enable_ft_package ............................... False
  enable_one_logger ............................... True
  encoder_num_layers .............................. 28
  encoder_pipeline_model_parallel_size ............ 0
  encoder_seq_length .............................. 2048
  encoder_tensor_model_parallel_size .............. 0
  end_weight_decay ................................ 0.1
  eod_mask_loss ................................... False
  error_injection_rate ............................ 0
  error_injection_type ............................ transient_error
  eval_interval ................................... 1000
  eval_iters ...................................... 0
  evidence_data_path .............................. None
  exit_duration_in_mins ........................... None
  exit_interval ................................... None
  exit_on_missing_checkpoint ...................... False
  exit_signal_handler ............................. False
  exit_trigger .................................... /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/logs/3b-softmax-flash-full-1n4-long-1245283/exit
  exp_avg_dtype ................................... torch.float32
  exp_avg_sq_dtype ................................ torch.float32
  expert_model_parallel_size ...................... 1
  expert_tensor_parallel_size ..................... 1
  ffn_hidden_size ................................. 8192
  final_layernorm ................................. True
  finetune ........................................ False
  fix_old_xielu ................................... False
  flash_decode .................................... False
  fp16 ............................................ False
  fp16_lm_cross_entropy ........................... False
  fp32_residual_connection ........................ False
  fp8 ............................................. None
  fp8_amax_compute_algo ........................... most_recent
  fp8_amax_history_len ............................ 1
  fp8_interval .................................... 1
  fp8_margin ...................................... 0
  fp8_param_gather ................................ False
  fp8_wgrad ....................................... True
  global_batch_size ............................... 32
  goldfish_h ...................................... 50
  goldfish_k ...................................... 50
  goldfish_loss ................................... False
  gradient_accumulation_fusion .................... True
  group_query_attention ........................... False
  head_lr_mult .................................... 1.0
  hidden_dropout .................................. 0.1
  hidden_size ..................................... 3072
  hierarchical_context_parallel_sizes ............. None
  hybrid_attention_ratio .......................... 0.0
  hybrid_mlp_ratio ................................ 0.0
  hybrid_override_pattern ......................... None
  hysteresis ...................................... 2
  ict_head_size ................................... None
  ict_load ........................................ None
  img_h ........................................... 224
  img_w ........................................... 224
  indexer_batch_size .............................. 128
  indexer_log_interval ............................ 1000
  inference_batch_times_seqlen_threshold .......... -1
  inference_max_seq_length ........................ 2560
  inference_rng_tracker ........................... False
  init_method_std ................................. 0.02
  init_method_xavier_uniform ...................... False
  initial_loss_scale .............................. 4294967296
  input_embeddings_multiplier ..................... 1.0
  iter_per_epoch .................................. 1250
  kv_channels ..................................... 128
  kv_lora_rank .................................... 32
  layernorm_init .................................. None
  lazy_mpu_init ................................... None
  load ............................................ None
  local_rank ...................................... 0
  log_interval .................................... 10
  log_loss_scale_to_tensorboard ................... True
  log_memory_to_tensorboard ....................... False
  log_num_zeros_in_grad ........................... False
  log_params_norm ................................. False
  log_params_norm_per_param ....................... False
  log_progress .................................... False
  log_straggler ................................... False
  log_throughput .................................. True
  log_timers_to_tensorboard ....................... False
  log_validation_ppl_to_tensorboard ............... False
  log_world_size_to_tensorboard ................... False
  logging_level ................................... None
  loss_scale ...................................... None
  loss_scale_window ............................... 1000
  lr .............................................. 0.00015
  lr_decay_iters .................................. None
  lr_decay_samples ................................ None
  lr_decay_style .................................. cosine
  lr_warmup_fraction .............................. None
  lr_warmup_init .................................. 0.0
  lr_warmup_iters ................................. 0
  lr_warmup_samples ............................... 0
  lr_wsd_decay_iters .............................. None
  lr_wsd_decay_samples ............................ None
  lr_wsd_decay_style .............................. 1-sqrt
  main_grads_dtype ................................ torch.float32
  main_params_dtype ............................... torch.float32
  make_vocab_size_divisible_by .................... 128
  manual_gc ....................................... False
  manual_gc_eval .................................. True
  manual_gc_interval .............................. 0
  mask_factor ..................................... 1.0
  mask_prob ....................................... 0.15
  mask_type ....................................... random
  masked_softmax_fusion ........................... True
  max_position_embeddings ......................... 2048
  max_tokens_to_oom ............................... 12000
  memory_snapshot_path ............................ snapshot.pickle
  merge_file ...................................... None
  micro_batch_size ................................ 1
  microbatch_group_size_per_vp_stage .............. None
  min_loss_scale .................................. 1.0
  min_lr .......................................... 1.5e-05
  mlp_layernorm ................................... True
  mmap_bin_files .................................. True
  mock_data ....................................... False
  moe_aux_loss_coeff .............................. 0.0
  moe_expert_capacity_factor ...................... None
  moe_extended_tp ................................. False
  moe_ffn_hidden_size ............................. 8192
  moe_grouped_gemm ................................ False
  moe_input_jitter_eps ............................ None
  moe_layer_freq .................................. 1
  moe_layer_recompute ............................. False
  moe_pad_expert_input_to_capacity ................ False
  moe_per_layer_logging ........................... False
  moe_router_bias_update_rate ..................... 0.001
  moe_router_enable_expert_bias ................... False
  moe_router_load_balancing_type .................. aux_loss
  moe_router_pre_softmax .......................... False
  moe_router_score_function ....................... softmax
  moe_router_topk ................................. 2
  moe_router_topk_limited_devices ................. None
  moe_router_topk_scaling_factor .................. None
  moe_shared_expert_intermediate_size ............. None
  moe_shared_expert_overlap ....................... False
  moe_token_dispatcher_type ....................... allgather
  moe_token_drop_policy ........................... probs
  moe_use_legacy_grouped_gemm ..................... False
  moe_use_upcycling ............................... False
  moe_z_loss_coeff ................................ None
  multi_latent_attention .......................... False
  nccl_communicator_config_path ................... None
  no_load_optim ................................... None
  no_load_rng ..................................... None
  no_persist_layer_norm ........................... False
  no_save_optim ................................... None
  no_save_rng ..................................... None
  non_persistent_ckpt_type ........................ None
  non_persistent_global_ckpt_dir .................. None
  non_persistent_local_ckpt_algo .................. fully_parallel
  non_persistent_local_ckpt_dir ................... None
  non_persistent_save_interval .................... None
  norm_epsilon .................................... 1e-05
  normalization ................................... LayerNorm
  num_attention_heads ............................. 24
  num_channels .................................... 3
  num_classes ..................................... 1000
  num_dataset_builder_threads ..................... 1
  num_distributed_optimizer_instances ............. 1
  num_experts ..................................... None
  num_layers ...................................... 28
  num_layers_per_virtual_pipeline_stage ........... None
  num_query_groups ................................ 1
  num_virtual_stages_per_pipeline_rank ............ None
  num_workers ..................................... 2
  one_logger_async ................................ False
  one_logger_project .............................. megatron-lm
  one_logger_run_name ............................. None
  onnx_safe ....................................... None
  openai_gelu ..................................... False
  optimizer ....................................... adam
  output_bert_embeddings .......................... False
  overlap_grad_reduce ............................. False
  overlap_p2p_comm ................................ False
  overlap_p2p_comm_warmup_flush ................... False
  overlap_param_gather ............................ False
  overlap_param_gather_with_optimizer_step ........ False
  override_opt_param_scheduler .................... False
  params_dtype .................................... torch.bfloat16
  patch_dim ....................................... 16
  per_split_data_args_path ........................ None
  perform_initialization .......................... True
  pipeline_model_parallel_size .................... 1
  pipeline_model_parallel_split_rank .............. None
  position_embedding_type ......................... learned_absolute
  post_layer_norm ................................. False
  pretrained_checkpoint ........................... None
  profile ......................................... False
  profile_ranks ................................... [0]
  profile_step_end ................................ 12
  profile_step_start .............................. 10
  q_lora_rank ..................................... None
  qk_head_dim ..................................... 128
  qk_layernorm .................................... False
  qk_pos_emb_head_dim ............................. 64
  qknorm_impl ..................................... te
  query_in_block_prob ............................. 0.1
  rampup_batch_size ............................... None
  rank ............................................ 0
  recompute_granularity ........................... None
  recompute_method ................................ None
  recompute_num_layers ............................ None
  record_memory_history ........................... False
  relative_attention_max_distance ................. 128
  relative_attention_num_buckets .................. 32
  replication ..................................... False
  replication_factor .............................. 2
  replication_jump ................................ None
  rerun_mode ...................................... disabled
  reset_attention_mask ............................ False
  reset_position_ids .............................. False
  retriever_report_topk_accuracies ................ []
  retriever_score_scaling ......................... False
  retriever_seq_length ............................ 256
  retro_add_retriever ............................. False
  retro_attention_gate ............................ 1
  retro_cyclic_train_iters ........................ None
  retro_encoder_attention_dropout ................. 0.1
  retro_encoder_hidden_dropout .................... 0.1
  retro_encoder_layers ............................ 2
  retro_num_neighbors ............................. 2
  retro_num_retrieved_chunks ...................... 2
  retro_project_dir ............................... None
  retro_verify_neighbor_count ..................... True
  rope_scaling_factor ............................. 8.0
  rotary_base ..................................... 10000
  rotary_interleaved .............................. False
  rotary_percent .................................. 1.0
  rotary_scaling_factor ........................... 1.0
  rotary_seq_len_interpolation_factor ............. None
  s3_cache_path ................................... None
  sample_rate ..................................... 1.0
  save ............................................ /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/checkpoints/3b-softmax-flash-full-1n4-long
  save_interval ................................... 1000
  save_trigger .................................... /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/logs/3b-softmax-flash-full-1n4-long-1245283/save
  scatter_gather_tensors_in_pipeline .............. True
  seed ............................................ 1234
  seq_length ...................................... 2048
  sequence_parallel ............................... False
  sgd_momentum .................................... 0.9
  short_seq_prob .................................. 0.1
  skip_train ...................................... False
  skipped_train_samples ........................... 0
  spec ............................................ None
  split ........................................... 100,0,0
  squared_relu .................................... False
  start_weight_decay .............................. 0.1
  straggler_ctrlr_port ............................ 65535
  straggler_minmax_count .......................... 1
  swiglu .......................................... False
  swin_backbone_type .............................. tiny
  te_rng_tracker .................................. False
  tensor_model_parallel_size ...................... 1
  tensorboard_dir ................................. /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/logs/3b-softmax-flash-full-1n4-long-1245283/tensorboard
  tensorboard_log_interval ........................ 1
  tensorboard_queue_size .......................... 1000
  test_data_path .................................. None
  test_mode ....................................... False
  tiktoken_num_special_tokens ..................... 1000
  tiktoken_pattern ................................ None
  tiktoken_special_tokens ......................... None
  timing_log_level ................................ 0
  timing_log_option ............................... minmax
  titles_data_path ................................ None
  tokenizer_model ................................. alehc/swissai-tokenizer
  tokenizer_type .................................. HuggingFaceTokenizer
  tp_comm_bootstrap_backend ....................... nccl
  tp_comm_bulk_dgrad .............................. True
  tp_comm_bulk_wgrad .............................. True
  tp_comm_overlap ................................. False
  tp_comm_overlap_ag .............................. True
  tp_comm_overlap_cfg ............................. None
  tp_comm_overlap_rs .............................. True
  tp_comm_overlap_rs_dgrad ........................ False
  tp_comm_split_ag ................................ True
  tp_comm_split_rs ................................ True
  train_data_path ................................. None
  train_iters ..................................... 60000
  train_samples ................................... None
  train_sync_interval ............................. None
  transformer_impl ................................ transformer_engine
  transformer_pipeline_model_parallel_size ........ 1
  trigger_path .................................... /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/logs/3b-softmax-flash-full-1n4-long-1245283
  untie_embeddings_and_output_weights ............. False
  use_checkpoint_args ............................. False
  use_checkpoint_opt_param_scheduler .............. False
  use_cpu_initialization .......................... None
  use_dist_ckpt ................................... True
  use_dist_ckpt_deprecated ........................ False
  use_distributed_optimizer ....................... False
  use_flash_attn .................................. True
  use_legacy_models ............................... False
  use_mp_args_from_checkpoint_args ................ False
  use_one_sent_docs ............................... False
  use_precision_aware_optimizer ................... False
  use_pytorch_profiler ............................ False
  use_ring_exchange_p2p ........................... False
  use_rope_scaling ................................ False
  use_rotary_position_embeddings .................. False
  use_tokenizer_model_from_checkpoint_args ........ True
  use_torch_fsdp2 ................................. False
  use_tp_pp_dp_mapping ............................ False
  v_head_dim ...................................... 128
  valid_data_path ................................. None
  variable_seq_lengths ............................ False
  virtual_pipeline_model_parallel_size ............ None
  vision_backbone_type ............................ vit
  vision_pretraining .............................. False
  vision_pretraining_type ......................... classify
  vocab_extra_ids ................................. 0
  vocab_file ...................................... None
  vocab_size ...................................... None
  wandb_exp_name .................................. 
  wandb_project ................................... 
  wandb_save_dir .................................. 
  weight_decay .................................... 0.1
  weight_decay_incr_style ......................... constant
  wgrad_deferral_limit ............................ 0
  world_size ...................................... 4
  xielu ........................................... False
  xiprelu ......................................... False
  xiprelup ........................................ False
  yaml_cfg ........................................ None
-------------------- end of arguments ---------------------
INFO:megatron.core.num_microbatches_calculator:setting number of microbatches to constant 8
> building HuggingFaceTokenizer tokenizer ...
 > padded vocab (size: 131072) with 0 dummy tokens (new size: 131072)
WARNING:megatron.core.rerun_state_machine:RerunStateMachine initialized in mode RerunMode.DISABLED
> initializing torch distributed ...
> setting tensorboard ...
WARNING: one_logger package is required to enable e2e metrics tracking. please go to https://confluence.nvidia.com/display/MLWFO/Package+Repositories for details to install it
[Gloo] Rank [Gloo] Rank [Gloo] Rank [Gloo] Rank 2013 is connected to  is connected to  is connected to  is connected to 3333 peer ranks.  peer ranks.  peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : Expected number of connected peer ranks is : Expected number of connected peer ranks is : 3333



[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3
[Gloo] Rank 2[Gloo] Rank  is connected to [Gloo] Rank 133 is connected to  peer ranks.  is connected to 3Expected number of connected peer ranks is : 3 peer ranks. 3 peer ranks. Expected number of connected peer ranks is : 
Expected number of connected peer ranks is : 33

[Gloo] Rank [Gloo] Rank 23 is connected to [Gloo] Rank [Gloo] Rank  is connected to 3013 peer ranks.  is connected to  is connected to  peer ranks. Expected number of connected peer ranks is : 33Expected number of connected peer ranks is : 3 peer ranks.  peer ranks. 3
Expected number of connected peer ranks is : Expected number of connected peer ranks is : 
33

> initialized tensor model parallel with size 1
> initialized pipeline model parallel with size 1
> setting random seeds to 1234 ...
> compiling dataset index builder ...
make: Entering directory '/iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/megatron/core/datasets'
make: Nothing to be done for 'default'.
make: Leaving directory '/iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/megatron/core/datasets'
>>> done with dataset index builder. Compilation time: 0.124 seconds
> compiling and loading fused kernels ...
NCCL version 2.28.8+cuda13.0
>>> done with compiling and loading fused kernels. Compilation time: 8.122 seconds
time to initialize megatron (seconds): 13.568
[after megatron is initialized] datetime: 2025-12-16 13:32:43 
building GPT model ...
Built model:
GPTModel(
  (embedding): LanguageModelEmbedding(
    (word_embeddings): VocabParallelEmbedding()
    (position_embeddings): Embedding(2048, 3072)
    (embedding_dropout): Dropout(p=0.1, inplace=False)
  )
  (decoder): TransformerBlock(
    (layers): ModuleList(
      (0-27): 28 x TransformerLayer(
        (input_layernorm): IdentityOp()
        (self_attention): SelfAttention(
          (core_attention): TEDotProductAttention(
            (flash_attention): FlashAttention()
            (fused_attention): FusedAttention()
            (unfused_attention): UnfusedDotProductAttention(
              (scale_mask_softmax): FusedScaleMaskSoftmax()
              (attention_dropout): Dropout(p=0.1, inplace=False)
            )
          )
          (linear_proj): TERowParallelLinear(in_features=3072, out_features=3072, bias=True, TP=1)
          (linear_qkv): TELayerNormColumnParallelLinear(in_features=3072, out_features=9216, bias=True, TP=1)
          (q_layernorm): IdentityOp()
          (k_layernorm): IdentityOp()
        )
        (pre_cross_attn_layernorm): IdentityOp()
        (cross_attention): IdentityOp()
        (cross_attn_bda): IdentityFuncOp()
        (pre_mlp_layernorm): IdentityOp()
        (mlp): MLP(
          (linear_fc1): TELayerNormColumnParallelLinear(in_features=3072, out_features=8192, bias=True, TP=1)
          (linear_fc2): TERowParallelLinear(in_features=8192, out_features=3072, bias=True, TP=1)
        )
      )
    )
    (final_layernorm): LayerNorm()
  )
  (output_layer): ColumnParallelLinear(in_features=3072, out_features=131072, bias=False, TP=1)
)
Config:
TransformerConfig(tensor_model_parallel_size=1, pipeline_model_parallel_size=1, virtual_pipeline_model_parallel_size=None, sequence_parallel=False, context_parallel_size=1, hierarchical_context_parallel_sizes=None, expert_model_parallel_size=1, expert_tensor_parallel_size=1, moe_extended_tp=False, perform_initialization=True, use_cpu_initialization=None, fp16=False, bf16=True, params_dtype=torch.bfloat16, timers=None, finalize_model_grads_func=None, grad_scale_func=None, no_sync_func=None, grad_sync_func=None, param_sync_func=None, deterministic_mode=False, enable_autocast=False, autocast_dtype=torch.bfloat16, num_microbatches_with_partial_activation_checkpoints=None, gradient_accumulation_fusion=True, async_tensor_model_parallel_allreduce=True, use_te_rng_tracker=False, tp_comm_overlap=False, tp_comm_bulk_wgrad=True, tp_comm_bulk_dgrad=True, tp_comm_overlap_ag=True, tp_comm_overlap_rs=True, tp_comm_overlap_rs_dgrad=False, tp_comm_split_ag=True, tp_comm_atomic_ag=False, tp_comm_split_rs=True, tp_comm_atomic_rs=False, cross_entropy_loss_fusion=False, tp_comm_overlap_disable_qkv=False, tp_comm_overlap_disable_fc1=False, tp_comm_bootstrap_backend='nccl', pipeline_dtype=torch.bfloat16, variable_seq_lengths=False, overlap_p2p_comm=False, batch_p2p_comm=True, batch_p2p_sync=True, use_ring_exchange_p2p=False, deallocate_pipeline_outputs=True, defer_embedding_wgrad_compute=False, wgrad_deferral_limit=0, pipeline_model_parallel_split_rank=None, overlap_p2p_comm_warmup_flush=False, microbatch_group_size_per_vp_stage=1, cpu_offloading=False, cpu_offloading_num_layers=0, _cpu_offloading_context=None, cpu_offloading_activations=True, cpu_offloading_weights=True, barrier_with_L1_time=True, num_layers=28, num_layers_in_first_pipeline_stage=None, num_layers_in_last_pipeline_stage=None, account_for_embedding_in_pipeline_split=False, account_for_loss_in_pipeline_split=False, hidden_size=3072, num_attention_heads=24, attention_backend=<AttnBackend.auto: 5>, softmax_scale=None, num_query_groups=24, ffn_hidden_size=8192, kv_channels=128, hidden_dropout=0.1, attention_dropout=0.1, fp32_residual_connection=False, apply_residual_connection_post_layernorm=False, layernorm_epsilon=1e-05, layernorm_zero_centered_gamma=False, add_bias_linear=True, add_qkv_bias=True, gated_linear_unit=False, activation_func=<built-in function gelu>, activation_func_fp8_input_store=False, num_moe_experts=None, rotary_interleaved=False, window_size=None, normalization='LayerNorm', qk_layernorm=False, test_mode=False, calculate_per_token_loss=False, multi_latent_attention=False, post_layer_norm=False, layernorm_init=None, init_method=<function init_method_normal.<locals>.init_ at 0x400417eb2520>, output_layer_init_method=<function scaled_init_method_normal.<locals>.init_ at 0x4005193f7ec0>, init_method_std=0.02, apply_query_key_layer_scaling=False, attention_softmax_in_fp32=False, bias_activation_fusion=True, masked_softmax_fusion=True, persist_layer_norm=True, memory_efficient_layer_norm=False, bias_dropout_fusion=True, apply_rope_fusion=False, recompute_granularity=None, recompute_method=None, recompute_num_layers=None, distribute_saved_activations=False, fp8=None, fp8_margin=0, fp8_interval=1, fp8_amax_history_len=1, fp8_amax_compute_algo='most_recent', fp8_wgrad=True, fp8_dot_product_attention=False, fp8_multi_head_attention=False, tp_only_amax_red=False, moe_shared_expert_intermediate_size=None, moe_shared_expert_overlap=False, moe_layer_freq=1, moe_ffn_hidden_size=8192, moe_router_load_balancing_type='aux_loss', moe_router_topk=2, moe_router_topk_limited_devices=None, moe_router_pre_softmax=False, moe_router_topk_scaling_factor=None, moe_router_score_function='softmax', moe_router_enable_expert_bias=False, moe_router_bias_update_rate=0.001, moe_grouped_gemm=False, moe_use_legacy_grouped_gemm=False, moe_aux_loss_coeff=0.0, moe_z_loss_coeff=None, moe_input_jitter_eps=None, moe_token_dropping=False, moe_token_dispatcher_type='allgather', moe_per_layer_logging=False, moe_expert_capacity_factor=None, moe_pad_expert_input_to_capacity=False, moe_token_drop_policy='probs', moe_layer_recompute=False, cp_comm_type='p2p', enable_cuda_graph=False, cuda_graph_use_single_mempool=False, cuda_graph_retain_backward_graph=False, cuda_graph_warmup_steps=2, external_cuda_graph=False, clone_scatter_output_in_embedding=True, disable_parameter_transpose_cache=False, config_logger_dir='', flash_decode=False, inference_rng_tracker=False)
 > number of parameters on (tensor, pipeline) model parallel rank (0, 0): 2876205056
INFO:megatron.core.distributed.distributed_data_parallel:Setting up DistributedDataParallel with config DistributedDataParallelConfig(grad_reduce_in_fp32=True, overlap_grad_reduce=False, overlap_param_gather=False, align_param_gather=False, use_distributed_optimizer=False, num_distributed_optimizer_instances=1, check_for_nan_in_grad=True, bucket_size=None, average_in_collective=False, fp8_param_gather=False)
INFO:megatron.core.distributed.param_and_grad_buffer:Number of buckets for gradient all-reduce / reduce-scatter: 1
Params for bucket 1 (2876205056 elements):
	module.decoder.layers.25.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.20.self_attention.linear_proj.bias
	module.decoder.layers.16.self_attention.linear_proj.bias
	module.decoder.layers.13.mlp.linear_fc2.bias
	module.decoder.layers.9.mlp.linear_fc2.bias
	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.5.mlp.linear_fc2.bias
	module.decoder.layers.4.self_attention.linear_proj.weight
	module.decoder.layers.1.mlp.linear_fc1.weight
	module.decoder.layers.0.self_attention.linear_proj.weight
	module.decoder.layers.24.self_attention.linear_proj.bias
	module.decoder.layers.17.mlp.linear_fc2.bias
	module.decoder.layers.13.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.9.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.3.self_attention.linear_qkv.bias
	module.decoder.layers.5.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.8.self_attention.linear_proj.weight
	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.1.mlp.linear_fc2.weight
	module.decoder.layers.25.mlp.linear_fc2.bias
	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.17.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.3.self_attention.linear_qkv.weight
	module.decoder.layers.13.mlp.linear_fc1.bias
	module.decoder.layers.9.mlp.linear_fc1.bias
	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.12.self_attention.linear_proj.weight
	module.decoder.layers.5.mlp.linear_fc1.bias
	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.25.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.21.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.2.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.17.mlp.linear_fc1.bias
	module.decoder.layers.11.self_attention.linear_qkv.bias
	module.decoder.layers.7.self_attention.linear_qkv.bias
	module.decoder.layers.20.self_attention.linear_proj.weight
	module.decoder.layers.16.self_attention.linear_proj.weight
	module.decoder.layers.13.mlp.linear_fc1.weight
	module.decoder.layers.9.mlp.linear_fc1.weight
	module.decoder.layers.5.mlp.linear_fc1.weight
	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.25.mlp.linear_fc1.bias
	module.decoder.layers.21.mlp.linear_fc2.bias
	module.decoder.layers.19.self_attention.linear_qkv.bias
	module.decoder.layers.24.self_attention.linear_proj.weight
	module.decoder.layers.17.mlp.linear_fc1.weight
	module.decoder.layers.11.self_attention.linear_qkv.weight
	module.decoder.layers.5.self_attention.linear_proj.bias
	module.decoder.layers.7.self_attention.linear_qkv.weight
	module.decoder.layers.9.mlp.linear_fc2.weight
	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.13.mlp.linear_fc2.weight
	module.decoder.layers.5.mlp.linear_fc2.weight
	module.decoder.layers.10.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.27.self_attention.linear_qkv.bias
	module.decoder.layers.23.self_attention.linear_qkv.bias
	module.decoder.layers.14.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.6.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.25.mlp.linear_fc1.weight
	module.decoder.layers.21.mlp.linear_fc1.bias
	module.decoder.layers.19.self_attention.linear_qkv.weight
	module.decoder.layers.15.self_attention.linear_qkv.bias
	module.decoder.layers.2.mlp.linear_fc2.bias
	module.decoder.layers.17.mlp.linear_fc2.weight
	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.18.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.27.self_attention.linear_qkv.weight
	module.decoder.layers.23.self_attention.linear_qkv.weight
	module.decoder.layers.9.self_attention.linear_proj.bias
	module.decoder.layers.2.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.25.mlp.linear_fc2.weight
	module.decoder.layers.13.self_attention.linear_proj.bias
	module.decoder.layers.26.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.22.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.21.self_attention.linear_proj.bias
	module.decoder.layers.17.self_attention.linear_proj.bias
	module.decoder.layers.14.mlp.linear_fc2.bias
	module.decoder.layers.10.mlp.linear_fc2.bias
	module.decoder.layers.4.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.6.mlp.linear_fc2.bias
	module.decoder.layers.2.mlp.linear_fc2.weight
	module.decoder.layers.5.self_attention.linear_proj.weight
	module.decoder.final_layernorm.bias
	module.decoder.layers.25.self_attention.linear_proj.bias
	module.decoder.layers.18.mlp.linear_fc2.bias
	module.decoder.layers.14.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.10.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.8.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.4.self_attention.linear_qkv.bias
	module.decoder.layers.6.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.2.mlp.linear_fc1.bias
	module.decoder.layers.0.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.26.mlp.linear_fc2.bias
	module.decoder.layers.22.mlp.linear_fc2.bias
	module.decoder.layers.18.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.12.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.4.self_attention.linear_qkv.weight
	module.decoder.layers.14.mlp.linear_fc1.bias
	module.decoder.layers.10.mlp.linear_fc1.bias
	module.decoder.layers.8.self_attention.linear_qkv.bias
	module.decoder.layers.13.self_attention.linear_proj.weight
	module.decoder.layers.9.self_attention.linear_proj.weight
	module.decoder.layers.6.mlp.linear_fc1.bias
	module.decoder.final_layernorm.weight
	module.decoder.layers.26.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.22.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.20.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.16.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.3.mlp.linear_fc1.layer_norm_weight
	module.embedding.word_embeddings.weight
	module.decoder.layers.18.mlp.linear_fc1.bias
	module.decoder.layers.12.self_attention.linear_qkv.bias
	module.decoder.layers.21.self_attention.linear_proj.weight
	module.decoder.layers.17.self_attention.linear_proj.weight
	module.decoder.layers.14.mlp.linear_fc1.weight
	module.decoder.layers.8.self_attention.linear_qkv.weight
	module.decoder.layers.10.mlp.linear_fc1.weight
	module.decoder.layers.2.self_attention.linear_proj.bias
	module.decoder.layers.6.mlp.linear_fc1.weight
	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.24.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.16.self_attention.linear_qkv.bias
	module.decoder.layers.26.mlp.linear_fc1.bias
	module.decoder.layers.22.mlp.linear_fc1.bias
	module.decoder.layers.20.self_attention.linear_qkv.bias
	module.decoder.layers.25.self_attention.linear_proj.weight
	module.decoder.layers.18.mlp.linear_fc1.weight
	module.decoder.layers.12.self_attention.linear_qkv.weight
	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.14.mlp.linear_fc2.weight
	module.decoder.layers.10.mlp.linear_fc2.weight
	module.decoder.layers.0.mlp.linear_fc2.bias
	module.decoder.layers.6.mlp.linear_fc2.weight
	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.24.self_attention.linear_qkv.bias
	module.decoder.layers.11.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.16.self_attention.linear_qkv.weight
	module.decoder.layers.3.mlp.linear_fc2.bias
	module.decoder.layers.7.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.26.mlp.linear_fc1.weight
	module.decoder.layers.22.mlp.linear_fc1.weight
	module.decoder.layers.20.self_attention.linear_qkv.weight
	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.18.mlp.linear_fc2.weight
	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.19.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.15.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.24.self_attention.linear_qkv.weight
	module.decoder.layers.10.self_attention.linear_proj.bias
	module.decoder.layers.22.mlp.linear_fc2.weight
	module.decoder.layers.26.mlp.linear_fc2.weight
	module.decoder.layers.14.self_attention.linear_proj.bias
	module.decoder.layers.3.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.6.self_attention.linear_proj.bias
	module.decoder.layers.27.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.23.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.2.self_attention.linear_proj.weight
	module.decoder.layers.18.self_attention.linear_proj.bias
	module.decoder.layers.11.mlp.linear_fc2.bias
	module.decoder.layers.7.mlp.linear_fc2.bias
	module.decoder.layers.3.mlp.linear_fc1.bias
	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.15.mlp.linear_fc2.bias
	module.decoder.layers.22.self_attention.linear_proj.bias
	module.decoder.layers.26.self_attention.linear_proj.bias
	module.decoder.layers.19.mlp.linear_fc2.bias
	module.decoder.layers.0.self_attention.linear_qkv.bias
	module.decoder.layers.11.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.5.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.7.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.3.mlp.linear_fc1.weight
	module.decoder.layers.27.mlp.linear_fc2.bias
	module.decoder.layers.23.mlp.linear_fc2.bias
	module.decoder.layers.19.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.15.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.13.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.9.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.0.self_attention.linear_qkv.weight
	module.decoder.layers.11.mlp.linear_fc1.bias
	module.decoder.layers.5.self_attention.linear_qkv.bias
	module.decoder.layers.3.mlp.linear_fc2.weight
	module.decoder.layers.14.self_attention.linear_proj.weight
	module.decoder.layers.10.self_attention.linear_proj.weight
	module.decoder.layers.7.mlp.linear_fc1.bias
	module.decoder.layers.1.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.6.self_attention.linear_proj.weight
	module.decoder.layers.27.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.23.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.21.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.17.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.4.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.19.mlp.linear_fc1.bias
	module.decoder.layers.15.mlp.linear_fc1.bias
	module.decoder.layers.13.self_attention.linear_qkv.bias
	module.decoder.layers.9.self_attention.linear_qkv.bias
	module.decoder.layers.5.self_attention.linear_qkv.weight
	module.decoder.layers.18.self_attention.linear_proj.weight
	module.decoder.layers.11.mlp.linear_fc1.weight
	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.7.mlp.linear_fc1.weight
	module.decoder.layers.25.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.8.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.17.self_attention.linear_qkv.bias
	module.decoder.layers.27.mlp.linear_fc1.bias
	module.decoder.layers.23.mlp.linear_fc1.bias
	module.decoder.layers.21.self_attention.linear_qkv.bias
	module.decoder.layers.1.self_attention.linear_proj.weight
	module.decoder.layers.15.mlp.linear_fc1.weight
	module.decoder.layers.9.self_attention.linear_qkv.weight
	module.decoder.layers.3.self_attention.linear_proj.bias
	module.decoder.layers.22.self_attention.linear_proj.weight
	module.decoder.layers.26.self_attention.linear_proj.weight
	module.decoder.layers.19.mlp.linear_fc1.weight
	module.decoder.layers.13.self_attention.linear_qkv.weight
	module.decoder.layers.0.mlp.linear_fc2.weight
	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.11.mlp.linear_fc2.weight
	module.decoder.layers.7.mlp.linear_fc2.weight
	module.decoder.layers.0.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.25.self_attention.linear_qkv.bias
	module.decoder.layers.12.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.17.self_attention.linear_qkv.weight
	module.decoder.layers.4.mlp.linear_fc2.bias
	module.decoder.layers.27.mlp.linear_fc1.weight
	module.decoder.layers.23.mlp.linear_fc1.weight
	module.decoder.layers.21.self_attention.linear_qkv.weight
	module.decoder.layers.15.mlp.linear_fc2.weight
	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.19.mlp.linear_fc2.weight
	module.decoder.layers.0.mlp.linear_fc1.bias
	module.decoder.layers.20.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.16.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.25.self_attention.linear_qkv.weight
	module.decoder.layers.11.self_attention.linear_proj.bias
	module.decoder.layers.4.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.8.mlp.linear_fc2.bias
	module.decoder.layers.27.mlp.linear_fc2.weight
	module.decoder.layers.23.mlp.linear_fc2.weight
	module.decoder.layers.15.self_attention.linear_proj.bias
	module.decoder.layers.2.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.7.self_attention.linear_proj.bias
	module.decoder.layers.24.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.1.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.1.self_attention.linear_proj.bias
	module.decoder.layers.19.self_attention.linear_proj.bias
	module.decoder.layers.12.mlp.linear_fc2.bias
	module.decoder.layers.21.mlp.linear_fc1.weight
	module.decoder.layers.8.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.4.mlp.linear_fc1.bias
	module.decoder.layers.2.self_attention.linear_qkv.bias
	module.decoder.layers.3.self_attention.linear_proj.weight
	module.decoder.layers.16.mlp.linear_fc2.bias
	module.decoder.layers.27.self_attention.linear_proj.bias
	module.decoder.layers.23.self_attention.linear_proj.bias
	module.decoder.layers.20.mlp.linear_fc2.bias
	module.decoder.layers.12.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.8.mlp.linear_fc1.bias
	module.decoder.layers.4.mlp.linear_fc1.weight
	module.decoder.layers.2.self_attention.linear_qkv.weight
	module.decoder.layers.3.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.24.mlp.linear_fc2.bias
	module.decoder.layers.20.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.16.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.14.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.10.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.12.mlp.linear_fc1.bias
	module.decoder.layers.6.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.4.mlp.linear_fc2.weight
	module.decoder.layers.15.self_attention.linear_proj.weight
	module.decoder.layers.11.self_attention.linear_proj.weight
	module.decoder.layers.8.mlp.linear_fc1.weight
	module.decoder.layers.0.self_attention.linear_proj.bias
	module.decoder.layers.7.self_attention.linear_proj.weight
	module.decoder.layers.24.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.18.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.20.mlp.linear_fc1.bias
	module.decoder.layers.16.mlp.linear_fc1.bias
	module.decoder.layers.14.self_attention.linear_qkv.bias
	module.decoder.layers.10.self_attention.linear_qkv.bias
	module.decoder.layers.6.self_attention.linear_qkv.bias
	module.decoder.layers.19.self_attention.linear_proj.weight
	module.decoder.layers.12.mlp.linear_fc1.weight
	module.decoder.layers.15.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.11.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.8.mlp.linear_fc2.weight
	module.decoder.layers.2.mlp.linear_fc1.weight
	module.embedding.position_embeddings.weight
	module.decoder.layers.26.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.22.self_attention.linear_qkv.layer_norm_bias
	module.decoder.layers.7.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.18.self_attention.linear_qkv.bias
	module.decoder.layers.24.mlp.linear_fc1.bias
	module.decoder.layers.14.self_attention.linear_qkv.weight
	module.decoder.layers.16.mlp.linear_fc1.weight
	module.decoder.layers.10.self_attention.linear_qkv.weight
	module.decoder.layers.4.self_attention.linear_proj.bias
	module.decoder.layers.23.self_attention.linear_proj.weight
	module.decoder.layers.27.self_attention.linear_proj.weight
	module.decoder.layers.20.mlp.linear_fc1.weight
	module.decoder.layers.1.mlp.linear_fc2.bias
	module.decoder.layers.6.self_attention.linear_qkv.weight
	module.decoder.layers.19.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.12.mlp.linear_fc2.weight
	module.decoder.layers.21.mlp.linear_fc2.weight
	module.decoder.layers.15.self_attention.linear_qkv.weight
	module.decoder.layers.26.self_attention.linear_qkv.bias
	module.decoder.layers.22.self_attention.linear_qkv.bias
	module.decoder.layers.13.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.9.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.0.mlp.linear_fc1.weight
	module.decoder.layers.18.self_attention.linear_qkv.weight
	module.decoder.layers.5.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.24.mlp.linear_fc1.weight
	module.decoder.layers.16.mlp.linear_fc2.weight
	module.decoder.layers.27.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.23.self_attention.linear_qkv.layer_norm_weight
	module.decoder.layers.20.mlp.linear_fc2.weight
	module.decoder.layers.8.self_attention.linear_proj.bias
	module.decoder.layers.1.mlp.linear_fc1.layer_norm_bias
	module.decoder.layers.1.self_attention.linear_qkv.weight
	module.decoder.layers.21.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.17.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.22.self_attention.linear_qkv.weight
	module.decoder.layers.26.self_attention.linear_qkv.weight
	module.decoder.layers.0.mlp.linear_fc1.layer_norm_weight
	module.decoder.layers.24.mlp.linear_fc2.weight
	module.decoder.layers.12.self_attention.linear_proj.bias
	module.decoder.layers.1.mlp.linear_fc1.bias
	module.decoder.layers.1.self_attention.linear_qkv.bias
INFO:megatron.core.optimizer:Setting up optimizer with config OptimizerConfig(optimizer='adam', lr=0.00015, min_lr=1.5e-05, decoupled_lr=None, decoupled_min_lr=None, weight_decay=0.1, fp16=False, bf16=True, params_dtype=torch.bfloat16, use_precision_aware_optimizer=False, main_grads_dtype=torch.float32, main_params_dtype=torch.float32, exp_avg_dtype=torch.float32, exp_avg_sq_dtype=torch.float32, loss_scale=None, initial_loss_scale=4294967296, min_loss_scale=1.0, loss_scale_window=1000, hysteresis=2, adam_beta1=0.9, adam_beta2=0.999, adam_eps=1e-08, sgd_momentum=0.9, ademamix_alpha=5, ademamix_beta3=0.999, ademamix_beta3_warmup=-1, ademamix_alpha_warmup=-1, use_distributed_optimizer=False, overlap_param_gather_with_optimizer_step=False, clip_grad=1.0, log_num_zeros_in_grad=False, barrier_with_L1_time=True, timers=<megatron.core.timers.Timers object at 0x400417ba3470>, config_logger_dir='')
INFO:megatron.core.optimizer_param_scheduler:> learning rate decay style: cosine
[after model, optimizer, and learning rate scheduler are built] datetime: 2025-12-16 13:32:45 
> building train, validation, and test datasets ...
 > datasets target sizes (minimum size):
    train:      1920000
    validation: 0
    test:       0
INFO:megatron.core.datasets.blended_megatron_dataset_config:Let split_matrix = [(0, 1.0), None, None]
> building train, validation, and test datasets for GPT ...
INFO:megatron.core.datasets.blended_megatron_dataset_builder:Building GPTDataset splits with sizes=(1920000, 0, 0) and config=GPTDatasetConfig(random_seed=1234, sequence_length=2048, blend=(['/users/ooikonomou/scratch/Megatron-LM-new-attention/datasets/train_data_meg_text_document'], None), blend_per_split=None, split='100,0,0', split_matrix=[(0, 1.0), None, None], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<megatron.training.tokenizer.tokenizer._HuggingFaceTokenizer object at 0x400416c23650>, reset_position_ids=False, reset_attention_mask=False, eod_mask_loss=False, goldfish_loss=False, goldfish_k=50, goldfish_h=50, create_attention_mask=True, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)
INFO:megatron.core.datasets.indexed_dataset:Load the _IndexReader from /users/ooikonomou/scratch/Megatron-LM-new-attention/datasets/train_data_meg_text_document.idx
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence lengths
INFO:megatron.core.datasets.indexed_dataset:	Extract the sequence pointers
INFO:megatron.core.datasets.indexed_dataset:	Extract the document indices
INFO:megatron.core.datasets.indexed_dataset:> total number of sequences: 785906
INFO:megatron.core.datasets.indexed_dataset:> total number of documents: 785906
INFO:megatron.core.datasets.gpt_dataset:Build and save the GPTDataset train indices
INFO:megatron.core.datasets.gpt_dataset:> total number of samples: 2046903
INFO:megatron.core.datasets.gpt_dataset:> total number of epochs: 5
> finished creating GPT datasets ...
[after dataloaders are built] datetime: 2025-12-16 13:32:45 
done with setup ...
(min, max) time across ranks (ms):
    model-and-optimizer-setup ......................: (2148.14, 2150.39)
    train/valid/test-data-iterators-setup ..........: (243.06, 348.89)
training ...
[before the start of training step] datetime: 2025-12-16 13:32:46 
NCCL version 2.28.8+cuda13.0
NCCL version 2.28.8+cuda13.0
NCCL version 2.28.8+cuda13.0
Number of parameters in transformer layers in billions:  2.47
Number of parameters in embedding layers in billions: 0.40
Total number of parameters in billions: 2.87
Number of parameters in most loaded shard in billions: 2.8693
Theoretical memory footprints: weight and optimizer=49254.01 MB
 [2025-12-16 13:33:16] iteration       10/   60000 | consumed samples:          320 | consumed tokens: 0.001B | elapsed time per iteration (ms): 3001.3 | eta: 2 days, 2:00:48 | tokens/sec/gpu: 5458.9 | throughput per GPU (TFLOP/s/GPU): 105.5 | learning rate: 1.500000E-04 | global batch size:    32 | lm loss: 1.234505E+01 | loss scale: 1.0 | grad norm: 4.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
[Rank 0] (after 10 iterations) memory (MB) | allocated: 49637.48486328125 | max allocated: 56029.75537109375 | reserved: 57086.0 | max reserved: 57086.0
 [2025-12-16 13:33:29] iteration       20/   60000 | consumed samples:          640 | consumed tokens: 0.001B | elapsed time per iteration (ms): 1319.1 | eta: 21:58:36 | tokens/sec/gpu: 12421.0 | throughput per GPU (TFLOP/s/GPU): 240.1 | learning rate: 1.500000E-04 | global batch size:    32 | lm loss: 8.331387E+00 | loss scale: 1.0 | grad norm: 2.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:33:42] iteration       30/   60000 | consumed samples:          960 | consumed tokens: 0.002B | elapsed time per iteration (ms): 1323.1 | eta: 22:02:24 | tokens/sec/gpu: 12383.4 | throughput per GPU (TFLOP/s/GPU): 239.3 | learning rate: 1.499999E-04 | global batch size:    32 | lm loss: 7.812792E+00 | loss scale: 1.0 | grad norm: 1.595 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:33:55] iteration       40/   60000 | consumed samples:         1280 | consumed tokens: 0.003B | elapsed time per iteration (ms): 1324.7 | eta: 22:03:47 | tokens/sec/gpu: 12368.3 | throughput per GPU (TFLOP/s/GPU): 239.0 | learning rate: 1.499998E-04 | global batch size:    32 | lm loss: 7.782848E+00 | loss scale: 1.0 | grad norm: 0.964 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:34:09] iteration       50/   60000 | consumed samples:         1600 | consumed tokens: 0.003B | elapsed time per iteration (ms): 1385.7 | eta: 23:04:32 | tokens/sec/gpu: 11823.6 | throughput per GPU (TFLOP/s/GPU): 228.5 | learning rate: 1.499998E-04 | global batch size:    32 | lm loss: 7.724972E+00 | loss scale: 1.0 | grad norm: 0.810 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:34:22] iteration       60/   60000 | consumed samples:         1920 | consumed tokens: 0.004B | elapsed time per iteration (ms): 1322.1 | eta: 22:00:49 | tokens/sec/gpu: 12392.0 | throughput per GPU (TFLOP/s/GPU): 239.5 | learning rate: 1.499997E-04 | global batch size:    32 | lm loss: 7.593485E+00 | loss scale: 1.0 | grad norm: 0.973 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:34:36] iteration       70/   60000 | consumed samples:         2240 | consumed tokens: 0.005B | elapsed time per iteration (ms): 1327.7 | eta: 22:06:09 | tokens/sec/gpu: 12340.1 | throughput per GPU (TFLOP/s/GPU): 238.5 | learning rate: 1.499995E-04 | global batch size:    32 | lm loss: 7.507812E+00 | loss scale: 1.0 | grad norm: 1.019 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:34:49] iteration       80/   60000 | consumed samples:         2560 | consumed tokens: 0.005B | elapsed time per iteration (ms): 1329.9 | eta: 22:08:08 | tokens/sec/gpu: 12319.6 | throughput per GPU (TFLOP/s/GPU): 238.1 | learning rate: 1.499994E-04 | global batch size:    32 | lm loss: 7.407818E+00 | loss scale: 1.0 | grad norm: 2.661 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:35:03] iteration       90/   60000 | consumed samples:         2880 | consumed tokens: 0.006B | elapsed time per iteration (ms): 1384.6 | eta: 23:02:32 | tokens/sec/gpu: 11832.9 | throughput per GPU (TFLOP/s/GPU): 228.7 | learning rate: 1.499993E-04 | global batch size:    32 | lm loss: 7.324942E+00 | loss scale: 1.0 | grad norm: 1.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:35:16] iteration      100/   60000 | consumed samples:         3200 | consumed tokens: 0.007B | elapsed time per iteration (ms): 1323.7 | eta: 22:01:30 | tokens/sec/gpu: 12377.3 | throughput per GPU (TFLOP/s/GPU): 239.2 | learning rate: 1.499991E-04 | global batch size:    32 | lm loss: 7.233386E+00 | loss scale: 1.0 | grad norm: 2.366 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:35:29] iteration      110/   60000 | consumed samples:         3520 | consumed tokens: 0.007B | elapsed time per iteration (ms): 1321.2 | eta: 21:58:46 | tokens/sec/gpu: 12400.9 | throughput per GPU (TFLOP/s/GPU): 239.7 | learning rate: 1.499989E-04 | global batch size:    32 | lm loss: 7.126132E+00 | loss scale: 1.0 | grad norm: 2.167 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:35:42] iteration      120/   60000 | consumed samples:         3840 | consumed tokens: 0.008B | elapsed time per iteration (ms): 1324.4 | eta: 22:01:45 | tokens/sec/gpu: 12370.8 | throughput per GPU (TFLOP/s/GPU): 239.1 | learning rate: 1.499987E-04 | global batch size:    32 | lm loss: 7.032538E+00 | loss scale: 1.0 | grad norm: 1.653 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:35:56] iteration      130/   60000 | consumed samples:         4160 | consumed tokens: 0.009B | elapsed time per iteration (ms): 1368.7 | eta: 22:45:45 | tokens/sec/gpu: 11970.2 | throughput per GPU (TFLOP/s/GPU): 231.4 | learning rate: 1.499984E-04 | global batch size:    32 | lm loss: 6.918838E+00 | loss scale: 1.0 | grad norm: 1.800 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:36:09] iteration      140/   60000 | consumed samples:         4480 | consumed tokens: 0.009B | elapsed time per iteration (ms): 1322.9 | eta: 21:59:47 | tokens/sec/gpu: 12385.2 | throughput per GPU (TFLOP/s/GPU): 239.4 | learning rate: 1.499982E-04 | global batch size:    32 | lm loss: 6.838729E+00 | loss scale: 1.0 | grad norm: 1.247 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:36:23] iteration      150/   60000 | consumed samples:         4800 | consumed tokens: 0.010B | elapsed time per iteration (ms): 1323.0 | eta: 21:59:38 | tokens/sec/gpu: 12384.4 | throughput per GPU (TFLOP/s/GPU): 239.4 | learning rate: 1.499979E-04 | global batch size:    32 | lm loss: 6.782890E+00 | loss scale: 1.0 | grad norm: 1.916 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:36:36] iteration      160/   60000 | consumed samples:         5120 | consumed tokens: 0.010B | elapsed time per iteration (ms): 1325.6 | eta: 22:02:04 | tokens/sec/gpu: 12359.5 | throughput per GPU (TFLOP/s/GPU): 238.9 | learning rate: 1.499976E-04 | global batch size:    32 | lm loss: 6.725068E+00 | loss scale: 1.0 | grad norm: 1.132 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:36:50] iteration      170/   60000 | consumed samples:         5440 | consumed tokens: 0.011B | elapsed time per iteration (ms): 1384.5 | eta: 23:00:35 | tokens/sec/gpu: 11833.7 | throughput per GPU (TFLOP/s/GPU): 228.7 | learning rate: 1.499973E-04 | global batch size:    32 | lm loss: 6.705192E+00 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:37:03] iteration      180/   60000 | consumed samples:         5760 | consumed tokens: 0.012B | elapsed time per iteration (ms): 1343.4 | eta: 22:19:24 | tokens/sec/gpu: 12195.6 | throughput per GPU (TFLOP/s/GPU): 235.7 | learning rate: 1.499970E-04 | global batch size:    32 | lm loss: 6.619810E+00 | loss scale: 1.0 | grad norm: 1.458 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:37:16] iteration      190/   60000 | consumed samples:         6080 | consumed tokens: 0.012B | elapsed time per iteration (ms): 1330.5 | eta: 22:06:18 | tokens/sec/gpu: 12313.9 | throughput per GPU (TFLOP/s/GPU): 238.0 | learning rate: 1.499967E-04 | global batch size:    32 | lm loss: 6.652126E+00 | loss scale: 1.0 | grad norm: 1.846 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:37:30] iteration      200/   60000 | consumed samples:         6400 | consumed tokens: 0.013B | elapsed time per iteration (ms): 1306.0 | eta: 21:41:41 | tokens/sec/gpu: 12544.8 | throughput per GPU (TFLOP/s/GPU): 242.5 | learning rate: 1.499963E-04 | global batch size:    32 | lm loss: 6.606290E+00 | loss scale: 1.0 | grad norm: 1.522 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:37:43] iteration      210/   60000 | consumed samples:         6720 | consumed tokens: 0.014B | elapsed time per iteration (ms): 1344.1 | eta: 22:19:26 | tokens/sec/gpu: 12189.2 | throughput per GPU (TFLOP/s/GPU): 235.6 | learning rate: 1.499959E-04 | global batch size:    32 | lm loss: 6.559891E+00 | loss scale: 1.0 | grad norm: 0.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:37:56] iteration      220/   60000 | consumed samples:         7040 | consumed tokens: 0.014B | elapsed time per iteration (ms): 1348.7 | eta: 22:23:42 | tokens/sec/gpu: 12148.4 | throughput per GPU (TFLOP/s/GPU): 234.8 | learning rate: 1.499955E-04 | global batch size:    32 | lm loss: 6.545752E+00 | loss scale: 1.0 | grad norm: 1.111 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:38:10] iteration      230/   60000 | consumed samples:         7360 | consumed tokens: 0.015B | elapsed time per iteration (ms): 1322.6 | eta: 21:57:31 | tokens/sec/gpu: 12387.8 | throughput per GPU (TFLOP/s/GPU): 239.4 | learning rate: 1.499951E-04 | global batch size:    32 | lm loss: 6.505784E+00 | loss scale: 1.0 | grad norm: 1.046 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:38:23] iteration      240/   60000 | consumed samples:         7680 | consumed tokens: 0.016B | elapsed time per iteration (ms): 1321.1 | eta: 21:55:51 | tokens/sec/gpu: 12401.4 | throughput per GPU (TFLOP/s/GPU): 239.7 | learning rate: 1.499947E-04 | global batch size:    32 | lm loss: 6.479607E+00 | loss scale: 1.0 | grad norm: 1.254 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:38:36] iteration      250/   60000 | consumed samples:         8000 | consumed tokens: 0.016B | elapsed time per iteration (ms): 1341.8 | eta: 22:16:15 | tokens/sec/gpu: 12210.0 | throughput per GPU (TFLOP/s/GPU): 236.0 | learning rate: 1.499942E-04 | global batch size:    32 | lm loss: 6.474165E+00 | loss scale: 1.0 | grad norm: 0.930 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:38:50] iteration      260/   60000 | consumed samples:         8320 | consumed tokens: 0.017B | elapsed time per iteration (ms): 1375.9 | eta: 22:49:53 | tokens/sec/gpu: 11908.2 | throughput per GPU (TFLOP/s/GPU): 230.2 | learning rate: 1.499937E-04 | global batch size:    32 | lm loss: 6.439963E+00 | loss scale: 1.0 | grad norm: 0.695 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:39:03] iteration      270/   60000 | consumed samples:         8640 | consumed tokens: 0.018B | elapsed time per iteration (ms): 1324.7 | eta: 21:58:45 | tokens/sec/gpu: 12367.9 | throughput per GPU (TFLOP/s/GPU): 239.0 | learning rate: 1.499933E-04 | global batch size:    32 | lm loss: 6.404282E+00 | loss scale: 1.0 | grad norm: 1.102 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:39:16] iteration      280/   60000 | consumed samples:         8960 | consumed tokens: 0.018B | elapsed time per iteration (ms): 1317.4 | eta: 21:51:15 | tokens/sec/gpu: 12436.5 | throughput per GPU (TFLOP/s/GPU): 240.4 | learning rate: 1.499927E-04 | global batch size:    32 | lm loss: 6.367056E+00 | loss scale: 1.0 | grad norm: 0.772 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:39:30] iteration      290/   60000 | consumed samples:         9280 | consumed tokens: 0.019B | elapsed time per iteration (ms): 1315.6 | eta: 21:49:14 | tokens/sec/gpu: 12453.7 | throughput per GPU (TFLOP/s/GPU): 240.7 | learning rate: 1.499922E-04 | global batch size:    32 | lm loss: 6.392635E+00 | loss scale: 1.0 | grad norm: 0.794 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:39:44] iteration      300/   60000 | consumed samples:         9600 | consumed tokens: 0.020B | elapsed time per iteration (ms): 1401.5 | eta: 23:14:30 | tokens/sec/gpu: 11690.3 | throughput per GPU (TFLOP/s/GPU): 225.9 | learning rate: 1.499917E-04 | global batch size:    32 | lm loss: 6.368818E+00 | loss scale: 1.0 | grad norm: 0.696 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:39:57] iteration      310/   60000 | consumed samples:         9920 | consumed tokens: 0.020B | elapsed time per iteration (ms): 1323.2 | eta: 21:56:19 | tokens/sec/gpu: 12382.5 | throughput per GPU (TFLOP/s/GPU): 239.3 | learning rate: 1.499911E-04 | global batch size:    32 | lm loss: 6.335936E+00 | loss scale: 1.0 | grad norm: 1.000 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:40:10] iteration      320/   60000 | consumed samples:        10240 | consumed tokens: 0.021B | elapsed time per iteration (ms): 1320.3 | eta: 21:53:17 | tokens/sec/gpu: 12409.0 | throughput per GPU (TFLOP/s/GPU): 239.8 | learning rate: 1.499905E-04 | global batch size:    32 | lm loss: 6.336119E+00 | loss scale: 1.0 | grad norm: 0.713 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:40:23] iteration      330/   60000 | consumed samples:        10560 | consumed tokens: 0.022B | elapsed time per iteration (ms): 1313.6 | eta: 21:46:22 | tokens/sec/gpu: 12472.6 | throughput per GPU (TFLOP/s/GPU): 241.1 | learning rate: 1.499899E-04 | global batch size:    32 | lm loss: 6.326819E+00 | loss scale: 1.0 | grad norm: 0.798 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:40:37] iteration      340/   60000 | consumed samples:        10880 | consumed tokens: 0.022B | elapsed time per iteration (ms): 1362.5 | eta: 22:34:47 | tokens/sec/gpu: 12024.9 | throughput per GPU (TFLOP/s/GPU): 232.4 | learning rate: 1.499893E-04 | global batch size:    32 | lm loss: 6.305853E+00 | loss scale: 1.0 | grad norm: 0.620 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:40:50] iteration      350/   60000 | consumed samples:        11200 | consumed tokens: 0.023B | elapsed time per iteration (ms): 1332.8 | eta: 22:05:00 | tokens/sec/gpu: 12293.1 | throughput per GPU (TFLOP/s/GPU): 237.6 | learning rate: 1.499887E-04 | global batch size:    32 | lm loss: 6.270938E+00 | loss scale: 1.0 | grad norm: 1.007 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:41:03] iteration      360/   60000 | consumed samples:        11520 | consumed tokens: 0.024B | elapsed time per iteration (ms): 1317.4 | eta: 21:49:30 | tokens/sec/gpu: 12436.4 | throughput per GPU (TFLOP/s/GPU): 240.4 | learning rate: 1.499880E-04 | global batch size:    32 | lm loss: 6.273972E+00 | loss scale: 1.0 | grad norm: 0.953 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:41:17] iteration      370/   60000 | consumed samples:        11840 | consumed tokens: 0.024B | elapsed time per iteration (ms): 1322.3 | eta: 21:54:10 | tokens/sec/gpu: 12390.2 | throughput per GPU (TFLOP/s/GPU): 239.5 | learning rate: 1.499873E-04 | global batch size:    32 | lm loss: 6.260361E+00 | loss scale: 1.0 | grad norm: 1.108 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:41:30] iteration      380/   60000 | consumed samples:        12160 | consumed tokens: 0.025B | elapsed time per iteration (ms): 1385.8 | eta: 22:57:00 | tokens/sec/gpu: 11822.9 | throughput per GPU (TFLOP/s/GPU): 228.5 | learning rate: 1.499866E-04 | global batch size:    32 | lm loss: 6.240604E+00 | loss scale: 1.0 | grad norm: 1.103 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:41:44] iteration      390/   60000 | consumed samples:        12480 | consumed tokens: 0.026B | elapsed time per iteration (ms): 1336.5 | eta: 22:07:48 | tokens/sec/gpu: 12259.0 | throughput per GPU (TFLOP/s/GPU): 236.9 | learning rate: 1.499859E-04 | global batch size:    32 | lm loss: 6.248283E+00 | loss scale: 1.0 | grad norm: 1.206 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:41:57] iteration      400/   60000 | consumed samples:        12800 | consumed tokens: 0.026B | elapsed time per iteration (ms): 1315.9 | eta: 21:47:09 | tokens/sec/gpu: 12450.6 | throughput per GPU (TFLOP/s/GPU): 240.6 | learning rate: 1.499852E-04 | global batch size:    32 | lm loss: 6.213831E+00 | loss scale: 1.0 | grad norm: 0.851 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:42:10] iteration      410/   60000 | consumed samples:        13120 | consumed tokens: 0.027B | elapsed time per iteration (ms): 1317.7 | eta: 21:48:43 | tokens/sec/gpu: 12433.5 | throughput per GPU (TFLOP/s/GPU): 240.3 | learning rate: 1.499845E-04 | global batch size:    32 | lm loss: 6.220036E+00 | loss scale: 1.0 | grad norm: 1.237 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:42:24] iteration      420/   60000 | consumed samples:        13440 | consumed tokens: 0.028B | elapsed time per iteration (ms): 1386.2 | eta: 22:56:29 | tokens/sec/gpu: 11819.4 | throughput per GPU (TFLOP/s/GPU): 228.4 | learning rate: 1.499837E-04 | global batch size:    32 | lm loss: 6.189967E+00 | loss scale: 1.0 | grad norm: 0.894 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:42:37] iteration      430/   60000 | consumed samples:        13760 | consumed tokens: 0.028B | elapsed time per iteration (ms): 1335.0 | eta: 22:05:25 | tokens/sec/gpu: 12272.8 | throughput per GPU (TFLOP/s/GPU): 237.2 | learning rate: 1.499829E-04 | global batch size:    32 | lm loss: 6.150859E+00 | loss scale: 1.0 | grad norm: 0.776 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:42:51] iteration      440/   60000 | consumed samples:        14080 | consumed tokens: 0.029B | elapsed time per iteration (ms): 1318.5 | eta: 21:48:47 | tokens/sec/gpu: 12426.7 | throughput per GPU (TFLOP/s/GPU): 240.2 | learning rate: 1.499821E-04 | global batch size:    32 | lm loss: 6.152439E+00 | loss scale: 1.0 | grad norm: 0.700 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:43:04] iteration      450/   60000 | consumed samples:        14400 | consumed tokens: 0.029B | elapsed time per iteration (ms): 1319.7 | eta: 21:49:47 | tokens/sec/gpu: 12415.1 | throughput per GPU (TFLOP/s/GPU): 240.0 | learning rate: 1.499813E-04 | global batch size:    32 | lm loss: 6.149220E+00 | loss scale: 1.0 | grad norm: 0.852 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:43:18] iteration      460/   60000 | consumed samples:        14720 | consumed tokens: 0.030B | elapsed time per iteration (ms): 1384.3 | eta: 22:53:39 | tokens/sec/gpu: 11835.8 | throughput per GPU (TFLOP/s/GPU): 228.8 | learning rate: 1.499804E-04 | global batch size:    32 | lm loss: 6.111718E+00 | loss scale: 1.0 | grad norm: 1.030 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:43:31] iteration      470/   60000 | consumed samples:        15040 | consumed tokens: 0.031B | elapsed time per iteration (ms): 1339.5 | eta: 22:09:00 | tokens/sec/gpu: 12231.4 | throughput per GPU (TFLOP/s/GPU): 236.4 | learning rate: 1.499796E-04 | global batch size:    32 | lm loss: 6.128746E+00 | loss scale: 1.0 | grad norm: 0.914 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:43:44] iteration      480/   60000 | consumed samples:        15360 | consumed tokens: 0.031B | elapsed time per iteration (ms): 1318.7 | eta: 21:48:08 | tokens/sec/gpu: 12424.4 | throughput per GPU (TFLOP/s/GPU): 240.1 | learning rate: 1.499787E-04 | global batch size:    32 | lm loss: 6.062469E+00 | loss scale: 1.0 | grad norm: 0.808 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:43:57] iteration      490/   60000 | consumed samples:        15680 | consumed tokens: 0.032B | elapsed time per iteration (ms): 1312.3 | eta: 21:41:36 | tokens/sec/gpu: 12484.7 | throughput per GPU (TFLOP/s/GPU): 241.3 | learning rate: 1.499778E-04 | global batch size:    32 | lm loss: 6.041599E+00 | loss scale: 1.0 | grad norm: 0.698 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:44:11] iteration      500/   60000 | consumed samples:        16000 | consumed tokens: 0.033B | elapsed time per iteration (ms): 1368.7 | eta: 22:37:16 | tokens/sec/gpu: 11970.6 | throughput per GPU (TFLOP/s/GPU): 231.4 | learning rate: 1.499769E-04 | global batch size:    32 | lm loss: 6.042930E+00 | loss scale: 1.0 | grad norm: 0.728 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:44:23] iteration      510/   60000 | consumed samples:        16320 | consumed tokens: 0.033B | elapsed time per iteration (ms): 1252.3 | eta: 20:41:39 | tokens/sec/gpu: 13083.1 | throughput per GPU (TFLOP/s/GPU): 252.9 | learning rate: 1.499759E-04 | global batch size:    32 | lm loss: 6.075240E+00 | loss scale: 1.0 | grad norm: 0.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:44:36] iteration      520/   60000 | consumed samples:        16640 | consumed tokens: 0.034B | elapsed time per iteration (ms): 1204.6 | eta: 19:54:10 | tokens/sec/gpu: 13601.1 | throughput per GPU (TFLOP/s/GPU): 262.9 | learning rate: 1.499750E-04 | global batch size:    32 | lm loss: 6.022376E+00 | loss scale: 1.0 | grad norm: 0.735 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:44:47] iteration      530/   60000 | consumed samples:        16960 | consumed tokens: 0.035B | elapsed time per iteration (ms): 1192.3 | eta: 19:41:46 | tokens/sec/gpu: 13741.3 | throughput per GPU (TFLOP/s/GPU): 265.6 | learning rate: 1.499740E-04 | global batch size:    32 | lm loss: 6.027223E+00 | loss scale: 1.0 | grad norm: 1.141 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:45:00] iteration      540/   60000 | consumed samples:        17280 | consumed tokens: 0.035B | elapsed time per iteration (ms): 1268.8 | eta: 20:57:22 | tokens/sec/gpu: 12913.1 | throughput per GPU (TFLOP/s/GPU): 249.6 | learning rate: 1.499730E-04 | global batch size:    32 | lm loss: 6.014608E+00 | loss scale: 1.0 | grad norm: 1.318 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:45:13] iteration      550/   60000 | consumed samples:        17600 | consumed tokens: 0.036B | elapsed time per iteration (ms): 1288.5 | eta: 21:16:40 | tokens/sec/gpu: 12715.7 | throughput per GPU (TFLOP/s/GPU): 245.8 | learning rate: 1.499720E-04 | global batch size:    32 | lm loss: 6.018382E+00 | loss scale: 1.0 | grad norm: 0.748 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:45:26] iteration      560/   60000 | consumed samples:        17920 | consumed tokens: 0.037B | elapsed time per iteration (ms): 1327.3 | eta: 21:54:53 | tokens/sec/gpu: 12344.1 | throughput per GPU (TFLOP/s/GPU): 238.6 | learning rate: 1.499710E-04 | global batch size:    32 | lm loss: 5.983397E+00 | loss scale: 1.0 | grad norm: 0.903 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:45:39] iteration      570/   60000 | consumed samples:        18240 | consumed tokens: 0.037B | elapsed time per iteration (ms): 1280.8 | eta: 21:08:36 | tokens/sec/gpu: 12792.2 | throughput per GPU (TFLOP/s/GPU): 247.2 | learning rate: 1.499699E-04 | global batch size:    32 | lm loss: 5.987812E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:45:53] iteration      580/   60000 | consumed samples:        18560 | consumed tokens: 0.038B | elapsed time per iteration (ms): 1356.5 | eta: 22:23:24 | tokens/sec/gpu: 12078.0 | throughput per GPU (TFLOP/s/GPU): 233.4 | learning rate: 1.499689E-04 | global batch size:    32 | lm loss: 5.972161E+00 | loss scale: 1.0 | grad norm: 0.660 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:46:05] iteration      590/   60000 | consumed samples:        18880 | consumed tokens: 0.039B | elapsed time per iteration (ms): 1281.1 | eta: 21:08:31 | tokens/sec/gpu: 12788.8 | throughput per GPU (TFLOP/s/GPU): 247.2 | learning rate: 1.499678E-04 | global batch size:    32 | lm loss: 5.961389E+00 | loss scale: 1.0 | grad norm: 0.906 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:46:19] iteration      600/   60000 | consumed samples:        19200 | consumed tokens: 0.039B | elapsed time per iteration (ms): 1330.9 | eta: 21:57:38 | tokens/sec/gpu: 12310.1 | throughput per GPU (TFLOP/s/GPU): 237.9 | learning rate: 1.499667E-04 | global batch size:    32 | lm loss: 5.990556E+00 | loss scale: 1.0 | grad norm: 0.704 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:46:32] iteration      610/   60000 | consumed samples:        19520 | consumed tokens: 0.040B | elapsed time per iteration (ms): 1284.4 | eta: 21:11:23 | tokens/sec/gpu: 12755.7 | throughput per GPU (TFLOP/s/GPU): 246.5 | learning rate: 1.499656E-04 | global batch size:    32 | lm loss: 5.945655E+00 | loss scale: 1.0 | grad norm: 0.838 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:46:45] iteration      620/   60000 | consumed samples:        19840 | consumed tokens: 0.041B | elapsed time per iteration (ms): 1359.5 | eta: 22:25:29 | tokens/sec/gpu: 12051.2 | throughput per GPU (TFLOP/s/GPU): 232.9 | learning rate: 1.499644E-04 | global batch size:    32 | lm loss: 5.920144E+00 | loss scale: 1.0 | grad norm: 0.760 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:46:58] iteration      630/   60000 | consumed samples:        20160 | consumed tokens: 0.041B | elapsed time per iteration (ms): 1284.8 | eta: 21:11:17 | tokens/sec/gpu: 12752.3 | throughput per GPU (TFLOP/s/GPU): 246.5 | learning rate: 1.499633E-04 | global batch size:    32 | lm loss: 5.893269E+00 | loss scale: 1.0 | grad norm: 0.712 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:47:11] iteration      640/   60000 | consumed samples:        20480 | consumed tokens: 0.042B | elapsed time per iteration (ms): 1326.9 | eta: 21:52:46 | tokens/sec/gpu: 12347.3 | throughput per GPU (TFLOP/s/GPU): 238.6 | learning rate: 1.499621E-04 | global batch size:    32 | lm loss: 5.897224E+00 | loss scale: 1.0 | grad norm: 0.819 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:47:24] iteration      650/   60000 | consumed samples:        20800 | consumed tokens: 0.043B | elapsed time per iteration (ms): 1282.8 | eta: 21:08:52 | tokens/sec/gpu: 12772.4 | throughput per GPU (TFLOP/s/GPU): 246.9 | learning rate: 1.499609E-04 | global batch size:    32 | lm loss: 5.873663E+00 | loss scale: 1.0 | grad norm: 0.824 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:47:38] iteration      660/   60000 | consumed samples:        21120 | consumed tokens: 0.043B | elapsed time per iteration (ms): 1344.2 | eta: 22:09:22 | tokens/sec/gpu: 12189.0 | throughput per GPU (TFLOP/s/GPU): 235.6 | learning rate: 1.499597E-04 | global batch size:    32 | lm loss: 5.905881E+00 | loss scale: 1.0 | grad norm: 0.865 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:47:50] iteration      670/   60000 | consumed samples:        21440 | consumed tokens: 0.044B | elapsed time per iteration (ms): 1285.0 | eta: 21:10:40 | tokens/sec/gpu: 12750.0 | throughput per GPU (TFLOP/s/GPU): 246.4 | learning rate: 1.499585E-04 | global batch size:    32 | lm loss: 5.903458E+00 | loss scale: 1.0 | grad norm: 0.757 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:48:04] iteration      680/   60000 | consumed samples:        21760 | consumed tokens: 0.045B | elapsed time per iteration (ms): 1332.4 | eta: 21:57:17 | tokens/sec/gpu: 12296.6 | throughput per GPU (TFLOP/s/GPU): 237.7 | learning rate: 1.499572E-04 | global batch size:    32 | lm loss: 5.849836E+00 | loss scale: 1.0 | grad norm: 0.730 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:48:17] iteration      690/   60000 | consumed samples:        22080 | consumed tokens: 0.045B | elapsed time per iteration (ms): 1288.7 | eta: 21:13:55 | tokens/sec/gpu: 12713.1 | throughput per GPU (TFLOP/s/GPU): 245.7 | learning rate: 1.499560E-04 | global batch size:    32 | lm loss: 5.816512E+00 | loss scale: 1.0 | grad norm: 0.674 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:48:30] iteration      700/   60000 | consumed samples:        22400 | consumed tokens: 0.046B | elapsed time per iteration (ms): 1363.6 | eta: 22:27:41 | tokens/sec/gpu: 12015.2 | throughput per GPU (TFLOP/s/GPU): 232.2 | learning rate: 1.499547E-04 | global batch size:    32 | lm loss: 5.856927E+00 | loss scale: 1.0 | grad norm: 0.872 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:48:43] iteration      710/   60000 | consumed samples:        22720 | consumed tokens: 0.047B | elapsed time per iteration (ms): 1281.3 | eta: 21:06:07 | tokens/sec/gpu: 12787.1 | throughput per GPU (TFLOP/s/GPU): 247.1 | learning rate: 1.499534E-04 | global batch size:    32 | lm loss: 5.873629E+00 | loss scale: 1.0 | grad norm: 0.788 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:48:56] iteration      720/   60000 | consumed samples:        23040 | consumed tokens: 0.047B | elapsed time per iteration (ms): 1283.3 | eta: 21:07:51 | tokens/sec/gpu: 12767.5 | throughput per GPU (TFLOP/s/GPU): 246.8 | learning rate: 1.499520E-04 | global batch size:    32 | lm loss: 5.799806E+00 | loss scale: 1.0 | grad norm: 0.809 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:49:09] iteration      730/   60000 | consumed samples:        23360 | consumed tokens: 0.048B | elapsed time per iteration (ms): 1325.6 | eta: 21:49:28 | tokens/sec/gpu: 12359.6 | throughput per GPU (TFLOP/s/GPU): 238.9 | learning rate: 1.499507E-04 | global batch size:    32 | lm loss: 5.815855E+00 | loss scale: 1.0 | grad norm: 0.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:49:23] iteration      740/   60000 | consumed samples:        23680 | consumed tokens: 0.048B | elapsed time per iteration (ms): 1363.9 | eta: 22:27:03 | tokens/sec/gpu: 12012.7 | throughput per GPU (TFLOP/s/GPU): 232.2 | learning rate: 1.499493E-04 | global batch size:    32 | lm loss: 5.817121E+00 | loss scale: 1.0 | grad norm: 0.678 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:49:36] iteration      750/   60000 | consumed samples:        24000 | consumed tokens: 0.049B | elapsed time per iteration (ms): 1284.5 | eta: 21:08:28 | tokens/sec/gpu: 12754.9 | throughput per GPU (TFLOP/s/GPU): 246.5 | learning rate: 1.499480E-04 | global batch size:    32 | lm loss: 5.795928E+00 | loss scale: 1.0 | grad norm: 0.778 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:49:49] iteration      760/   60000 | consumed samples:        24320 | consumed tokens: 0.050B | elapsed time per iteration (ms): 1290.3 | eta: 21:13:58 | tokens/sec/gpu: 12697.7 | throughput per GPU (TFLOP/s/GPU): 245.4 | learning rate: 1.499466E-04 | global batch size:    32 | lm loss: 5.813080E+00 | loss scale: 1.0 | grad norm: 0.867 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:50:02] iteration      770/   60000 | consumed samples:        24640 | consumed tokens: 0.050B | elapsed time per iteration (ms): 1341.2 | eta: 22:03:58 | tokens/sec/gpu: 12216.0 | throughput per GPU (TFLOP/s/GPU): 236.1 | learning rate: 1.499451E-04 | global batch size:    32 | lm loss: 5.772723E+00 | loss scale: 1.0 | grad norm: 0.780 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:50:16] iteration      780/   60000 | consumed samples:        24960 | consumed tokens: 0.051B | elapsed time per iteration (ms): 1376.3 | eta: 22:38:27 | tokens/sec/gpu: 11904.0 | throughput per GPU (TFLOP/s/GPU): 230.1 | learning rate: 1.499437E-04 | global batch size:    32 | lm loss: 5.786188E+00 | loss scale: 1.0 | grad norm: 0.680 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:50:29] iteration      790/   60000 | consumed samples:        25280 | consumed tokens: 0.052B | elapsed time per iteration (ms): 1293.9 | eta: 21:16:53 | tokens/sec/gpu: 12662.2 | throughput per GPU (TFLOP/s/GPU): 244.7 | learning rate: 1.499423E-04 | global batch size:    32 | lm loss: 5.789439E+00 | loss scale: 1.0 | grad norm: 0.779 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:50:42] iteration      800/   60000 | consumed samples:        25600 | consumed tokens: 0.052B | elapsed time per iteration (ms): 1295.0 | eta: 21:17:44 | tokens/sec/gpu: 12651.7 | throughput per GPU (TFLOP/s/GPU): 244.5 | learning rate: 1.499408E-04 | global batch size:    32 | lm loss: 5.710363E+00 | loss scale: 1.0 | grad norm: 0.723 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:50:55] iteration      810/   60000 | consumed samples:        25920 | consumed tokens: 0.053B | elapsed time per iteration (ms): 1341.1 | eta: 22:03:01 | tokens/sec/gpu: 12216.6 | throughput per GPU (TFLOP/s/GPU): 236.1 | learning rate: 1.499393E-04 | global batch size:    32 | lm loss: 5.718386E+00 | loss scale: 1.0 | grad norm: 0.835 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:51:09] iteration      820/   60000 | consumed samples:        26240 | consumed tokens: 0.054B | elapsed time per iteration (ms): 1342.9 | eta: 22:04:33 | tokens/sec/gpu: 12200.3 | throughput per GPU (TFLOP/s/GPU): 235.8 | learning rate: 1.499378E-04 | global batch size:    32 | lm loss: 5.729449E+00 | loss scale: 1.0 | grad norm: 0.693 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:51:22] iteration      830/   60000 | consumed samples:        26560 | consumed tokens: 0.054B | elapsed time per iteration (ms): 1303.6 | eta: 21:25:31 | tokens/sec/gpu: 12568.6 | throughput per GPU (TFLOP/s/GPU): 242.9 | learning rate: 1.499363E-04 | global batch size:    32 | lm loss: 5.729060E+00 | loss scale: 1.0 | grad norm: 0.768 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:51:34] iteration      840/   60000 | consumed samples:        26880 | consumed tokens: 0.055B | elapsed time per iteration (ms): 1284.9 | eta: 21:06:53 | tokens/sec/gpu: 12751.4 | throughput per GPU (TFLOP/s/GPU): 246.5 | learning rate: 1.499347E-04 | global batch size:    32 | lm loss: 5.703908E+00 | loss scale: 1.0 | grad norm: 0.814 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:51:48] iteration      850/   60000 | consumed samples:        27200 | consumed tokens: 0.056B | elapsed time per iteration (ms): 1335.1 | eta: 21:56:12 | tokens/sec/gpu: 12271.6 | throughput per GPU (TFLOP/s/GPU): 237.2 | learning rate: 1.499332E-04 | global batch size:    32 | lm loss: 5.697735E+00 | loss scale: 1.0 | grad norm: 0.980 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:52:01] iteration      860/   60000 | consumed samples:        27520 | consumed tokens: 0.056B | elapsed time per iteration (ms): 1320.0 | eta: 21:41:02 | tokens/sec/gpu: 12412.4 | throughput per GPU (TFLOP/s/GPU): 239.9 | learning rate: 1.499316E-04 | global batch size:    32 | lm loss: 5.745987E+00 | loss scale: 1.0 | grad norm: 0.833 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:52:14] iteration      870/   60000 | consumed samples:        27840 | consumed tokens: 0.057B | elapsed time per iteration (ms): 1268.1 | eta: 20:49:44 | tokens/sec/gpu: 12919.9 | throughput per GPU (TFLOP/s/GPU): 249.7 | learning rate: 1.499300E-04 | global batch size:    32 | lm loss: 5.702441E+00 | loss scale: 1.0 | grad norm: 0.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:52:26] iteration      880/   60000 | consumed samples:        28160 | consumed tokens: 0.058B | elapsed time per iteration (ms): 1253.4 | eta: 20:34:58 | tokens/sec/gpu: 13072.1 | throughput per GPU (TFLOP/s/GPU): 252.6 | learning rate: 1.499284E-04 | global batch size:    32 | lm loss: 5.711143E+00 | loss scale: 1.0 | grad norm: 0.841 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:52:39] iteration      890/   60000 | consumed samples:        28480 | consumed tokens: 0.058B | elapsed time per iteration (ms): 1253.2 | eta: 20:34:34 | tokens/sec/gpu: 13074.2 | throughput per GPU (TFLOP/s/GPU): 252.7 | learning rate: 1.499267E-04 | global batch size:    32 | lm loss: 5.677139E+00 | loss scale: 1.0 | grad norm: 0.668 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:52:52] iteration      900/   60000 | consumed samples:        28800 | consumed tokens: 0.059B | elapsed time per iteration (ms): 1367.0 | eta: 22:26:32 | tokens/sec/gpu: 11984.9 | throughput per GPU (TFLOP/s/GPU): 231.6 | learning rate: 1.499251E-04 | global batch size:    32 | lm loss: 5.663575E+00 | loss scale: 1.0 | grad norm: 0.685 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:53:05] iteration      910/   60000 | consumed samples:        29120 | consumed tokens: 0.060B | elapsed time per iteration (ms): 1291.0 | eta: 21:11:25 | tokens/sec/gpu: 12690.9 | throughput per GPU (TFLOP/s/GPU): 245.3 | learning rate: 1.499234E-04 | global batch size:    32 | lm loss: 5.693932E+00 | loss scale: 1.0 | grad norm: 0.682 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:53:18] iteration      920/   60000 | consumed samples:        29440 | consumed tokens: 0.060B | elapsed time per iteration (ms): 1273.6 | eta: 20:54:02 | tokens/sec/gpu: 12864.7 | throughput per GPU (TFLOP/s/GPU): 248.6 | learning rate: 1.499217E-04 | global batch size:    32 | lm loss: 5.679139E+00 | loss scale: 1.0 | grad norm: 0.866 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:53:31] iteration      930/   60000 | consumed samples:        29760 | consumed tokens: 0.061B | elapsed time per iteration (ms): 1272.8 | eta: 20:53:06 | tokens/sec/gpu: 12872.1 | throughput per GPU (TFLOP/s/GPU): 248.8 | learning rate: 1.499200E-04 | global batch size:    32 | lm loss: 5.674756E+00 | loss scale: 1.0 | grad norm: 0.853 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:53:45] iteration      940/   60000 | consumed samples:        30080 | consumed tokens: 0.062B | elapsed time per iteration (ms): 1397.1 | eta: 22:55:13 | tokens/sec/gpu: 11727.0 | throughput per GPU (TFLOP/s/GPU): 226.7 | learning rate: 1.499183E-04 | global batch size:    32 | lm loss: 5.652551E+00 | loss scale: 1.0 | grad norm: 0.785 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:53:57] iteration      950/   60000 | consumed samples:        30400 | consumed tokens: 0.062B | elapsed time per iteration (ms): 1272.1 | eta: 20:51:55 | tokens/sec/gpu: 12879.8 | throughput per GPU (TFLOP/s/GPU): 248.9 | learning rate: 1.499165E-04 | global batch size:    32 | lm loss: 5.654316E+00 | loss scale: 1.0 | grad norm: 0.864 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:54:10] iteration      960/   60000 | consumed samples:        30720 | consumed tokens: 0.063B | elapsed time per iteration (ms): 1273.9 | eta: 20:53:32 | tokens/sec/gpu: 12861.0 | throughput per GPU (TFLOP/s/GPU): 248.6 | learning rate: 1.499147E-04 | global batch size:    32 | lm loss: 5.635978E+00 | loss scale: 1.0 | grad norm: 0.789 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:54:23] iteration      970/   60000 | consumed samples:        31040 | consumed tokens: 0.064B | elapsed time per iteration (ms): 1275.2 | eta: 20:54:34 | tokens/sec/gpu: 12848.3 | throughput per GPU (TFLOP/s/GPU): 248.3 | learning rate: 1.499130E-04 | global batch size:    32 | lm loss: 5.618039E+00 | loss scale: 1.0 | grad norm: 0.883 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:54:37] iteration      980/   60000 | consumed samples:        31360 | consumed tokens: 0.064B | elapsed time per iteration (ms): 1391.1 | eta: 22:48:21 | tokens/sec/gpu: 11777.9 | throughput per GPU (TFLOP/s/GPU): 227.6 | learning rate: 1.499112E-04 | global batch size:    32 | lm loss: 5.588613E+00 | loss scale: 1.0 | grad norm: 0.917 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:54:49] iteration      990/   60000 | consumed samples:        31680 | consumed tokens: 0.065B | elapsed time per iteration (ms): 1263.6 | eta: 20:42:42 | tokens/sec/gpu: 12966.5 | throughput per GPU (TFLOP/s/GPU): 250.6 | learning rate: 1.499093E-04 | global batch size:    32 | lm loss: 5.583336E+00 | loss scale: 1.0 | grad norm: 0.761 | number of skipped iterations:   0 | number of nan iterations:   0 |
 [2025-12-16 13:55:02] iteration     1000/   60000 | consumed samples:        32000 | consumed tokens: 0.066B | elapsed time per iteration (ms): 1265.9 | eta: 20:44:46 | tokens/sec/gpu: 12942.8 | throughput per GPU (TFLOP/s/GPU): 250.2 | learning rate: 1.499075E-04 | global batch size:    32 | lm loss: 5.603880E+00 | loss scale: 1.0 | grad norm: 0.847 | number of skipped iterations:   0 | number of nan iterations:   0 |
saving checkpoint at iteration    1000 to /iopsstor/scratch/cscs/ooikonomou/Megatron-LM-new-attention/checkpoints/3b-softmax-flash-full-1n4-long in torch_dist format
===== DONE: Softmax Flash Full 1N4G LONG =====
